{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "MNIST Model Creation\n",
    "***\n",
    "\n",
    "*By Jared Kelnhofer, 10-02-2020*\n",
    "\n",
    "The purpose of this portion of the project is to create a model that can accurately categorize handwritten digits. This model will then be used in the main body of the project. I'll be using the MNIST handwritten digit dataset, and saving it in the /Dataset directory which is in the same directory as this notebook. TODO: add a way to load the local dataset instead of downloading it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import os\n",
    "if not os.path.exists('./Dataset'):\n",
    "    os.makedirs('./Dataset')\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, data_home=\"./Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll check out the MNIST dataset, and gain some insight on how it is structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n",
      "Shape of \"data\" key: (70000, 784)\n",
      "Shape of \"target\" key: (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.keys())\n",
    "print(\"Shape of \\\"data\\\" key: \" + str(np.shape(mnist.data)))\n",
    "print(\"Shape of \\\"target\\\" key: \" + str(np.shape(mnist.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sicne the square root of 784 is 28, I'm going to assume that the images are 28 by 28 pixels. I'm not sure why they're not stored in a 3d vector, but it doesn't really matter. Numpy has a function \"reshape\" that will let me change the shape of the 1D vector representing any image into a 28 by 28 matrix, and thus view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_image default shape is: (784,)\n",
      "test_image new shape is: (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n",
      "highest pixel value: 255.0\n",
      "lowest pixel value: 0.0\n",
      "First pixel amount: 0.0\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, 70000)\n",
    "\n",
    "test_image = mnist.data[index]\n",
    "print(\"test_image default shape is: \" + str(np.shape(test_image)))\n",
    "test_image = test_image.reshape(28, 28)\n",
    "print(\"test_image new shape is: \" + str(np.shape(test_image)))\n",
    "\n",
    "\n",
    "plt.imshow(test_image)\n",
    "plt.show()\n",
    "\n",
    "print(\"Label: \" + mnist.target[index])\n",
    "\n",
    "print(\"highest pixel value: \" + str(np.max(mnist.data[index])))\n",
    "print(\"lowest pixel value: \" + str(np.min(mnist.data[index])))\n",
    "print(\"First pixel amount: \" + str(mnist.data[index][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split up the data into the training and test sets. MNIST has the last 10,000 images set aside as testing images, and the same goes for the target labels. MNIST comes pre-shuffled, so I don't have to worry about stratified splitting. (Note: we also know now that the images have 0 as the background and high values as the number.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "X_test shape: (10000, 784)\n",
      "Y_train shape: (60000,)\n",
      "Y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = mnist.data[:60000]\n",
    "X_test = mnist.data[60000:]\n",
    "Y_train = mnist.target[:60000]\n",
    "Y_test = mnist.target[60000:]\n",
    "\n",
    "print(\"X_train shape: \" + str(np.shape(X_train)))\n",
    "print(\"X_test shape: \" + str(np.shape(X_test)))\n",
    "print(\"Y_train shape: \" + str(np.shape(Y_train)))\n",
    "print(\"Y_test shape: \" + str(np.shape(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all I need to do is create a model that can categorize the data with at least 97% accuracy, and test it to make sure I am not overfitting. Then I'll save the model, and move on to the rest of this project! This is where I'll add my hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well that worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aight, let's see a confusion matrix that compares the actual labels of our test data to the model's predictions. While we're at it, let's check out the accuracy and loss of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[ 914    0   10    6    5   13   10    5    9    8]\n",
      " [   3 1089    8    8    2    3    7    5    9    1]\n",
      " [   7    8  895   30   15   11   10   25   24    7]\n",
      " [   7    6   30  864    5   41    6    5   24   22]\n",
      " [   7    4    4    7  861    7   14   14   23   41]\n",
      " [  15    6    3   45   10  742   24    5   27   15]\n",
      " [  17    6   12    7   18   19  853    2   21    3]\n",
      " [   2   12   22   16    8    5    2  931    5   25]\n",
      " [  12    9   30   41   25   30   13    9  783   22]\n",
      " [  16    5   12   18   42   13    4   22   22  855]]\n",
      "accuracy:0.8787\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(Y_test, Y_predict))\n",
    "\n",
    "print(\"accuracy:\" + str(tree.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good enough! Now I'll save the model with pickle, and test re-loading it into the system. I'll also use the score() method to find out what my mean accuracy is on the image recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model to: /home/admin/Desktop/DigitRecognizer/Dev/Model/trained_tree.pickle\n",
      "loading model!\n",
      "mean accuracy: 0.8787\n"
     ]
    }
   ],
   "source": [
    "this_files_directory = os.getcwd()\n",
    "filename = this_files_directory + '/trained_tree.pickle'\n",
    "pickle.dump(tree, open(filename, 'wb'))\n",
    "\n",
    "print(\"saving model to: \" + str(filename))\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "print(\"loading model!\")\n",
    "print(\"mean accuracy: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is saved, and I know I can load it, I'll move onto the other portion of this project -- the GUI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
